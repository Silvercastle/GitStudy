`24년 3월 19일 스터디
============

> TensorFlow 1.x 버전을 사용하기 위해 호환성 모듈을 불러옵니다   
> TensorFlow 2.x의 기능을 사용하지 않도록 설정합니다.
```
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
```
> x축 입력 데이터   
> y축 출력 데이터   
```
xData = [1,2,3,4,5,6,7]
yData = [25000,55000,75000, 110000, 128000, 155000, 180000]
```
> Weight를 랜덤 초기화합니다.   
> Bias를 랜덤 초기화합니다.   
> 입력값을 담을 Placeholder를 생성합니다.   
> 출력값을 담을 Placeholder를 생성합니다. 
```
W = tf.Variable(tf.random.uniform([1], -100, 100))
b = tf.Variable(tf.random.uniform([1], -100, 100))
X = tf.placeholder(tf.float32)
Y = tf.placeholder(tf.float32)
```
> 가설 함수를 정의합니다:    
> H = W*X + b   
> 손실 함수를 정의합니다: 평균 제곱 오차   
> 학습률을 설정합니다.   
```
H = W * X + b
cost = tf.reduce_mean(tf.square(H-Y))
a = tf.Variable(0.01)
```
> Gradient Descent Optimizer를 정의합니다.   
> 최적화 과정을 정의합니다: 손실 함수를 최소화하는 방향으로 학습합니다.
> 변수들을 초기화하기 위한 연산을 정의합니다.
```
optimizer = tf.train.GradientDescentOptimizer(a)
train = optimizer.minimize(cost)
init = tf.global_variables_initializer()
```
> 세션을 생성합니다.
> 변수들을 초기화합니다.
```
sess = tf.Session()
sess.run(init)
```
> 5000번의 반복 학습을 수행합니다.   
> 학습 연산을 실행하고 입력과 출력 데이터를 전달합니다.  
```
for i in range(5001):
    sess.run(train, feed_dict={X: xData, Y: yData})
    if i % 500 == 0:
        print(i, sess.run(cost, feed_dict={X: xData, Y: yData}), sess.run(W), sess.run(b))
```
> 500번의 학습마다 현재 손실값과 파라미터(W, b)를 출력합니다. 
```
0 4611716600.0 [10347.233] [2053.1636]
500 8686738.0 [25537.863] [1948.4888]
1000 8633799.0 [25627.557] [1504.3354]
1500 8632678.0 [25640.627] [1439.6149]
2000 8632660.0 [25642.533] [1430.1809]
2500 8632649.0 [25642.807] [1428.8192]
3000 8632650.0 [25642.844] [1428.6328]
3500 8632657.0 [25642.848] [1428.6133]
4000 8632657.0 [25642.848] [1428.6133]
4500 8632657.0 [25642.848] [1428.6133]
5000 8632657.0 [25642.848] [1428.6133]
```

> 학습된 모델을 통해 새로운 입력값 [8]에 대한 예측을 출력합니다.   
```
print(sess.run(H, feed_dict={X: [8]}))
```
> 8에 대한 예측값
```
[206571.39]
```
![학습값에 대한 선형회귀1](/KES/240319/image/image01.png)
![학습값에 대한 선형회귀2](/KES/240319/image/image02.png)